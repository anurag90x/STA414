Seed : 125
> square_error <- importance_sampling(x,y,testx,testy,square_cov)
[1] 4.881551e-88

> cat("Square error for 1000 samples")
Square error for 1000 samples
> print(square_error)
[1] 0.2416936


Square error for 1000 samples[1] 5.507953e-88
[1] 5.507953e-88
                                                 test replications elapsed relative
1 importance_sampling(x, y, testx, testy, square_cov)            1  436.28        1
  user.self sys.self user.child sys.child
1    435.53     0.02         NA        NA

> # Script using the Gaussian process regression functions from wk6funcs.r.
> 
> source("wk6funcs.r")

> training_data <- function(data)
+ {
+   
+   result <- read.table(data,head=FALSE)
+   
+   return(as.matrix(result))
+ }

> # Covariance function to use.  Takes a vector x1 (actually a scalar here)
> # and a matrix X2 (with only one column here) as arguments, and returns  .... [TRUNCATED] 

> absolute_cov<-function (x1,X2, h)
+ {
+   gamma <- h[2]
+   rho <- h[3]
+   result <- matrix(NA,1,nrow(X2))
+   difference <- t(t(X2)-x1)
+   rows<- .... [TRUNCATED] 

> combined_cov<-function (x1,X2, h)
+ {
+   gamma1 <- h[2]
+   gamma2 <- h[3]
+   rho <- h[4]
+   absResult <- matrix(NA,1,nrow(X2))
+   squareResult  .... [TRUNCATED] 

> find_square_error <- function(pred_y,test_y)
+ {
+   error <- 0
+   rows <- nrow(test_y)
+   error <- (pred_y-test_y)^2
+   return (mean(error))
+   .... [TRUNCATED] 

> cross_validation <- function(trainx,trainy,covf,hypers)
+ {
+   hyperVals <- numeric(hypers)
+   for (v in 1:10)
+   {
+     cat("\nUsing cases",(v- .... [TRUNCATED] 

> gp_try_hypers <- function(trainx,trainy,val.ix,covf,hypers0, ...)
+ {
+   model <- nlm(function(h) predict_values(trainx,trainy,val.ix,covf, h^2+0.0 .... [TRUNCATED] 

> predict_values<-function(trainx,trainy,val.ix,covf,hyperparam)
+ {
+   x <-trainx[-val.ix,] #training
+   y <- as.matrix(trainy[-val.ix])
+   testx< .... [TRUNCATED] 

> importance_sampling <-function(trainx,trainy,testx,testy,covf)
+ {
+   set.seed(225)
+   num_samples <- 1000
+   
+   noise_dist <- rnorm(num_sample .... [TRUNCATED] 

> scale_data <- function(xvalues)
+ {
+   xvalues[,1] <- xvalues[,1]/10
+   xvalues[,7] <- xvalues[,7]/10
+   xvalues
+   
+ }

> # load the data
> x <- training_data("train1x.txt")

> y <- training_data("train1y.txt")

> testx <- training_data("testx.txt")

> testy <- training_data("testy.txt")

> # Test 1 - Square error for square covariance function
> average_time_hyper <-benchmark(replications = 10,hyperparam <- gp_find_hypers (x, y, square .... [TRUNCATED] 
Maximum log likelihood: -197.8135 

> # Script using the Gaussian process regression functions from wk6funcs.r.
> 
> source("wk6funcs.r")

> training_data <- function(data)
+ {
+   
+   result <- read.table(data,head=FALSE)
+   
+   return(as.matrix(result))
+ }

> # Covariance function to use.  Takes a vector x1 (actually a scalar here)
> # and a matrix X2 (with only one column here) as arguments, and returns  .... [TRUNCATED] 

> absolute_cov<-function (x1,X2, h)
+ {
+   gamma <- h[2]
+   rho <- h[3]
+   result <- matrix(NA,1,nrow(X2))
+   difference <- t(t(X2)-x1)
+   rows<- .... [TRUNCATED] 

> combined_cov<-function (x1,X2, h)
+ {
+   gamma1 <- h[2]
+   gamma2 <- h[3]
+   rho <- h[4]
+   absResult <- matrix(NA,1,nrow(X2))
+   squareResult  .... [TRUNCATED] 

> find_square_error <- function(pred_y,test_y)
+ {
+   error <- 0
+   rows <- nrow(test_y)
+   error <- (pred_y-test_y)^2
+   return (mean(error))
+   .... [TRUNCATED] 

> cross_validation <- function(trainx,trainy,covf,hypers)
+ {
+   hyperVals <- numeric(hypers)
+   for (v in 1:10)
+   {
+     cat("\nUsing cases",(v- .... [TRUNCATED] 

> gp_try_hypers <- function(trainx,trainy,val.ix,covf,hypers0, ...)
+ {
+   model <- nlm(function(h) predict_values(trainx,trainy,val.ix,covf, h^2+0.0 .... [TRUNCATED] 

> predict_values<-function(trainx,trainy,val.ix,covf,hyperparam)
+ {
+   x <-trainx[-val.ix,] #training
+   y <- as.matrix(trainy[-val.ix])
+   testx< .... [TRUNCATED] 

> importance_sampling <-function(trainx,trainy,testx,testy,covf)
+ {
+   set.seed(225)
+   num_samples <- 1000
+   
+   noise_dist <- rnorm(num_sample .... [TRUNCATED] 

> scale_data <- function(xvalues)
+ {
+   xvalues[,1] <- xvalues[,1]/10
+   xvalues[,7] <- xvalues[,7]/10
+   xvalues
+   
+ }

> # load the data
> x <- training_data("train1x.txt")

> y <- training_data("train1y.txt")

> testx <- training_data("testx.txt")

> testy <- training_data("testy.txt")

> # Test 1 - Square error for square covariance function
> average_time_hyper <-benchmark(replications = 10,hyperparam <- gp_find_hypers (x, y, square .... [TRUNCATED] 
Maximum log likelihood: -197.8135 

> # Script using the Gaussian process regression functions from wk6funcs.r.
> 
> source("wk6funcs.r")

> training_data <- function(data)
+ {
+   
+   result <- read.table(data,head=FALSE)
+   
+   return(as.matrix(result))
+ }

> # Covariance function to use.  Takes a vector x1 (actually a scalar here)
> # and a matrix X2 (with only one column here) as arguments, and returns  .... [TRUNCATED] 

> absolute_cov<-function (x1,X2, h)
+ {
+   gamma <- h[2]
+   rho <- h[3]
+   result <- matrix(NA,1,nrow(X2))
+   difference <- t(t(X2)-x1)
+   rows<- .... [TRUNCATED] 

> combined_cov<-function (x1,X2, h)
+ {
+   gamma1 <- h[2]
+   gamma2 <- h[3]
+   rho <- h[4]
+   absResult <- matrix(NA,1,nrow(X2))
+   squareResult  .... [TRUNCATED] 

> find_square_error <- function(pred_y,test_y)
+ {
+   error <- 0
+   rows <- nrow(test_y)
+   error <- (pred_y-test_y)^2
+   return (mean(error))
+   .... [TRUNCATED] 

> cross_validation <- function(trainx,trainy,covf,hypers)
+ {
+   hyperVals <- numeric(hypers)
+   for (v in 1:10)
+   {
+     cat("\nUsing cases",(v- .... [TRUNCATED] 

> gp_try_hypers <- function(trainx,trainy,val.ix,covf,hypers0, ...)
+ {
+   model <- nlm(function(h) predict_values(trainx,trainy,val.ix,covf, h^2+0.0 .... [TRUNCATED] 

> predict_values<-function(trainx,trainy,val.ix,covf,hyperparam)
+ {
+   x <-trainx[-val.ix,] #training
+   y <- as.matrix(trainy[-val.ix])
+   testx< .... [TRUNCATED] 

> importance_sampling <-function(trainx,trainy,testx,testy,covf)
+ {
+   set.seed(225)
+   num_samples <- 1000
+   
+   noise_dist <- rnorm(num_sample .... [TRUNCATED] 

> scale_data <- function(xvalues)
+ {
+   xvalues[,1] <- xvalues[,1]/10
+   xvalues[,7] <- xvalues[,7]/10
+   xvalues
+   
+ }

> # load the data
> x <- training_data("train1x.txt")

> y <- training_data("train1y.txt")

> testx <- training_data("testx.txt")

> testy <- training_data("testy.txt")

> # Test 1 - Square error for square covariance function
> average_time_hyper <-benchmark(replications = 10,hyperparam <- gp_find_hypers (x, y, square .... [TRUNCATED] 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 
Maximum log likelihood: -197.8135 

> predicted_y <- gp_predict (x, y, testx,square_cov, hyperparam)

> square_error <- find_square_error(predicted_y,testy)
                                                        test replications elapsed
1 hyperparam <- gp_find_hypers(x, y, square_cov, c(4, 4, 4))           10   50.76
  relative user.self sys.self user.child sys.child
1        1      50.7        0         NA        NA
[1] 50.76
                                                        test replications elapsed
1 hyperparam <- gp_find_hypers(x, y, square_cov, c(4, 4, 4))           10   50.76
  relative user.self sys.self user.child sys.child
1        1      50.7        0         NA        NA
[1] 10
